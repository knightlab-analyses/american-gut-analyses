{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: J W Debelius<br>\n",
    "**email**: jdebelius@ucsd.edu<br>\n",
    "**Date**: June 2017<br>\n",
    "**enviroment**: agp_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will look at the relationship between metadata covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sn\n",
    "import skbio\n",
    "import sys\n",
    "from scipy.cluster.hierarchy import ward\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the mapping file and data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_ = pd.read_csv('./03.packaged/1250/ag_map_with_alpha.txt', sep='\\t', dtype=str)\n",
    "map_.set_index('#SampleID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = pd.read_csv('./03.packaged/data_dictionary.csv', dtype=str)\n",
    "data_dict.set_index('column_name', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few clean up steps in the data dictionary that are being pushed to the main repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict.loc[['exercise_location', 'taxon_id'], 'data type'] = 'str'\n",
    "data_dict.loc['collection_month', 'expected_values'] = '\"January\" | \"February\" | \"March\" | \"April\" | \"May\" | \"June\" | \"July\" | \"August\" | \"September\" | \"October\" | \"November\" | \"December\"'\n",
    "data_dict.loc['other_supplement_frequency', 'data type'] = 'bool'\n",
    "data_dict.loc['other_supplement_frequency', 'question_type'] = np.nan\n",
    "data_dict.loc['deodorant_use', 'question_type'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set up a type conversion, so we can get the data type appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_lookup = {'str': str,\n",
    "               'string': str,\n",
    "               'free text': str,\n",
    "               'bool': str,\n",
    "               'int': float,\n",
    "               'float': float,\n",
    "               'datetime: MM/DD/YYYY': 'datetime',\n",
    "               'datetime: YYYY': 'datetime',\n",
    "               'datetime: MM/DD/YYYY HH:MM': 'datetime',\n",
    "               'datetime': 'datetime',\n",
    "               }\n",
    "\n",
    "def birth_year(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    else:\n",
    "        return int(float(x))\n",
    "\n",
    "def date_convert(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    else:\n",
    "        return pd.to_datetime(x)\n",
    "    \n",
    "def replace_quotes(exp):\n",
    "    name_space = exp.replace('\\'', '').split(' | ')\n",
    "    return {v: i for i, v in enumerate(name_space)}\n",
    "    \n",
    "datetime_mod = {'birth_year': birth_year,\n",
    "                'collection_date': date_convert,\n",
    "                'collection_time': date_convert,\n",
    "                'collection_timestamp': date_convert,\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to drop out ambigious values, and standardize boolean values (just in case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll replace missing or ambigious values\n",
    "map_.replace(['Unspecified', 'not applicable', 'Missing: Not collected', 'Not sure'],\n",
    "              np.nan, inplace=True)\n",
    "#  We'll also clean up some languistic differences in ordinal variables\n",
    "map_.replace('Rarely (less than once/week)', 'Rarely (a few times/month)', inplace=True)\n",
    "map_.replace('True', 'Yes', inplace=True)\n",
    "map_.replace('False', 'No', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several columns we want to skip, either because we know they'll be redundant (i.e. `assigned_from_geo` and `country` are already known to be redundant), because they're unique for each sample (`anonymized_name`), or because they're constant for all values (e.g. `body_site`, `altitude`).\n",
    "\n",
    "We'll also ignore all sequencing-related information, like the sequencing depth and alpha diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_skip = {'age_units',\n",
    "                   'alcohol_types_unspecified',\n",
    "                   'allergic_to_unspecified',\n",
    "                   'altitude',\n",
    "                   'alzheimers',\n",
    "                   'anonymized_name',\n",
    "                   'assigned_from_geo',\n",
    "                   'barcode',\n",
    "                   'body_habitat',\n",
    "                   'body_product',\n",
    "                   'body_site',\n",
    "                   'center_name',\n",
    "                   'comments_renamed',\n",
    "                   'condition_renamed',\n",
    "                   'depression_bipolar_schizophrenia',\n",
    "                   'depth',\n",
    "                   'description',\n",
    "                   'diabetes_type',\n",
    "                   'dna_extracted',\n",
    "                   'env_biome',\n",
    "                   'env_feature',\n",
    "                   'env_material',\n",
    "                   'env_package',\n",
    "                   'experiment_center',\n",
    "                   'experiment_title',\n",
    "                   'faiths_pd_1250',\n",
    "                   'has_physical_specimen',\n",
    "                   'height_units',\n",
    "                   'host_common_name',\n",
    "                   'host_subject_id',\n",
    "                   'host_taxid',\n",
    "                   'ibd_diagnosis',\n",
    "                   'ibd_diagnosis_refined',\n",
    "                   'instrument_model',\n",
    "                   'library_construction_protocol',\n",
    "                   'linker',\n",
    "                   'mental_illness_type_schizophrenia',\n",
    "                   'mental_illness_type_substance_abuse',\n",
    "                   'mental_illness_type_unspecified',\n",
    "                   'non_food_allergies_unspecified',\n",
    "                   'observed_otus_1250',\n",
    "                   'orig_name',\n",
    "                   'pcr_primers',\n",
    "                   'physical_specimen_location',\n",
    "                   'physical_specimen_remaining',\n",
    "                   'pku',\n",
    "                   'plateid',\n",
    "                   'platform',\n",
    "                   'public',\n",
    "                   'qiita_study_id',\n",
    "                   'samp_size',\n",
    "                   'sample_center',\n",
    "                   'sample_name',\n",
    "                   'sample_type',\n",
    "                   'scientific_name',\n",
    "                   'seq_depth',\n",
    "                   'sequencing_meth',\n",
    "                   'shannon_1250',\n",
    "                   'specialized_diet_exclude_nightshades',\n",
    "                   'specialized_diet_fodmap',\n",
    "                   'specialized_diet_halaal',\n",
    "                   'specialized_diet_kosher',\n",
    "                   'specialized_diet_raw_food_diet',\n",
    "                   'specialized_diet_unspecified',\n",
    "                   'survey_id',\n",
    "                   'target_gene',\n",
    "                   'target_subfragment',\n",
    "                   'taxon_id',\n",
    "                   'thyroid',\n",
    "                   'title',\n",
    "                   'tm1000_8_tool',\n",
    "                   'vioscreen_bcodeid',\n",
    "                   'vioscreen_calcium',\n",
    "                   'vioscreen_calcium_freq',\n",
    "                   'vioscreen_database',\n",
    "                   'vioscreen_finished',\n",
    "                   'vioscreen_isomalt',\n",
    "                   'vioscreen_lactitol',\n",
    "                   'vioscreen_maltitol',\n",
    "                   'vioscreen_multi_calcium_dose',\n",
    "                   'vioscreen_multivitamin',\n",
    "                   'vioscreen_multivitamin_freq',\n",
    "                   'vioscreen_procdate',\n",
    "                   'vioscreen_protocol',\n",
    "                   'vioscreen_questionnaire',\n",
    "                   'vioscreen_sacchar',\n",
    "                   'vioscreen_started',\n",
    "                   'vioscreen_sucrlose',\n",
    "                   'weight_units',\n",
    "                   'well_description'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also make a list of the all the columns in the prep template, which describe the experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prep_columns = {'center_project_name', \n",
    "                'experiment_design_description',\n",
    "                'extractionkit_lot', \n",
    "                'instrument_model',\n",
    "                'library_construction_protocol',\n",
    "                'mastermix_lot', \n",
    "                'pcr_primers',\n",
    "                'plating',\n",
    "                'primer',\n",
    "                'primer_date',\n",
    "                'project_name',\n",
    "                'qiita_prep_id', \n",
    "                'sample_plate',\n",
    "                'run_center',\n",
    "                'center_project_name', \n",
    "                'experiment_design_description',\n",
    "                'extractionkit_lot', \n",
    "                'instrument_model',\n",
    "                'library_construction_protocol',\n",
    "                'mastermix_lot', \n",
    "                'pcr_primers',\n",
    "                'plating',\n",
    "                'primer',\n",
    "                'primer_date',\n",
    "                'project_name',\n",
    "                'qiita_prep_id', \n",
    "                'sample_plate',\n",
    "                'run_center',\n",
    "                'run_date',\n",
    "                'run_prefix',\n",
    "                'tm50_8_tool',\n",
    "                'tm300_8_tool',\n",
    "                'tm1000_8_tool',\n",
    "                'water_lot',\n",
    "                'well',\n",
    "                'well_id','run_date',\n",
    "                'run_prefix',\n",
    "                'tm50_8_tool',\n",
    "                'tm300_8_tool',\n",
    "                'tm1000_8_tool',\n",
    "                'water_lot',\n",
    "                'well',\n",
    "                'well_id',\n",
    "                'extraction_robot',\n",
    "                'processing_robot',\n",
    "                'center_project_name',\n",
    "                'experiment_design_description',\n",
    "                'primer_plate',\n",
    "                'primerplate_renamed',\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be to categorize variables as continous, datetime, ordinal, or categorical. We'll classify any variable as categorical if it can be cast to a number, or it's a vioscreen column. We'll then convert all these columns to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continous = [c for c in data_dict.index \n",
    "             if ((data_dict.loc[c, 'data type'] in {'int', 'float'})\n",
    "             and c not in columns_to_skip)]\n",
    "vioscreen = [c for c in map_.columns if ('vioscreen' in c) and (c not in columns_to_skip)]\n",
    "\n",
    "continous.extend(vioscreen)\n",
    "continous.extend(['birth_year', 'height_corrected', 'weight_corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_[continous] = map_[continous].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 columns that can be treated as datetime values; we'll convert these values to a date time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetime = ['collection_date', 'collection_time', 'collection_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_date(x):\n",
    "    if pd.isnull(x):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return (pd.to_datetime(x) - pd.to_datetime('01-01-2012')).total_seconds()\n",
    "for c in datetime:\n",
    "    map_[c] = map_[c].apply(convert_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll choose to clean up some of the columns. We'll combine low frequency groups that can logically be combined into single groups. For instance, we have very few people who sleep less than 5 hours, so we can combine this with people who sleep 5 to 6 hours and make a single category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_up = {\n",
    "    \"antibiotic_history\": {\"Week\": \"Month\"},\n",
    "    'sleep_duration': {'Less than 5 hours': 'Less than 6', \n",
    "                       '5-6 hours': 'Less than 6'},\n",
    "    'bowel_movement_frequency': {'Four': 'Four or more', \n",
    "                                 'Five or more': 'Four or more'},\n",
    "    'diet_type': {'Vegan': 'Vegetarian'},\n",
    "    'last_move': {'Within the past month': 'Within the past 3 months'},\n",
    "    'pool_frequency': {'Occasionally (1-2 times/week)': 'Weekly',\n",
    "                       'Regularly (3-5 times/week)': \"Weekly\", \n",
    "                       'Daily': \"Weekly\"},\n",
    "    'smoking_frequency': {'Occasionally (1-2 times/week)': 'Weekly',\n",
    "                          'Regularly (3-5 times/week)': \"Weekly\", \n",
    "                          'Daily': \"Weekly\"},\n",
    "    'sugar_sweetened_drink_frequency': {'Occasionally (1-2 times/week)': 'Weekly',\n",
    "                                        'Regularly (3-5 times/week)': \"Weekly\", \n",
    "                                        'Daily': \"Weekly\"},\n",
    "    'vivid_dreams': {'Occasionally (1-2 times/week)': 'Weekly',\n",
    "                     'Regularly (3-5 times/week)': \"Weekly\", \n",
    "                      'Daily': \"Weekly\"},\n",
    "    'frozen_dessert_frequency': {'Occasionally (1-2 times/week)': 'Weekly',\n",
    "                                 'Regularly (3-5 times/week)': \"Weekly\", \n",
    "                                 'Daily': \"Weekly\"},\n",
    "    'vegetable_frequency': {'Never': \"Less than weekly\",\n",
    "                            'Rarely (less than once/week)': \"Less than weekly\"},\n",
    "    'extraction_robot': {'HowE': 'HOWE',\n",
    "                         'HOWE_KF1': 'HOWE',\n",
    "                         'HOWE_KF2': 'HOWE',\n",
    "                         'HOWE_KF3': 'HOWE',\n",
    "                         'HOWE_KF4': 'HOWE',\n",
    "                         'NewE': 'NEWE',\n",
    "                         'CARMEN_KF1': 'CARMEN',\n",
    "                         'CARMEN_KF2': 'CARMEN',\n",
    "                         'CARMEN_KF3': 'CARMEN',\n",
    "                         'CARMEN_KF4': 'CARMEN'},\n",
    "    'processing_robot': {'JerE': 'JERE',\n",
    "                         'RikE': 'RIKE',\n",
    "                         'RobE': 'ROBE'},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleans up the ordinal data\n",
    "map_.replace(clean_up, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume anything that can be treated as a frequency is ordinal. We'll also add a set of values that are ordinal (i.e. the categories have an intrensic order.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordinal = set(data_dict.index[data_dict['question_type'] == 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordinal = ordinal.union({\n",
    "    'antibiotic_history',\n",
    "    'age_cat',\n",
    "    'bmi_cat',\n",
    "    'bowel_movement_frequency',\n",
    "    'collection_month',\n",
    "    'collection_season',\n",
    "    'diet_type',\n",
    "    'drinks_per_session',\n",
    "    'flu_vaccine_date',\n",
    "    'last_move',\n",
    "    'last_travel',\n",
    "    'level_of_education',\n",
    "    'roommates',\n",
    "    'sleep_duration',\n",
    "    'types_of_plants',\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything that is a string or boolean value and isn't ordinal is assumed to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [c for c in data_dict.index if ((data_dict.loc[c, 'data type'] in {'str', 'bool'}) and \n",
    "                                              (c not in columns_to_skip) and \n",
    "                                              (c not in ordinal))]\n",
    "categorical.extend(list(prep_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert the ordinal variables to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ordinal_assignment(exp, replace=None):\n",
    "    \"\"\"Converts an ordered list of varible to integer values\"\"\"\n",
    "    if replace is not None:\n",
    "        def remap(x):\n",
    "            if x in replace:\n",
    "                return replace[x]\n",
    "            else:\n",
    "                return x\n",
    "    else:\n",
    "        def remap(x):\n",
    "            return x\n",
    "    order = []\n",
    "    for o in exp.replace('\"', '').split(' | '):\n",
    "        new_o = remap(o)\n",
    "        if (new_o not in order) and (not pd.isnull(new_o)):\n",
    "            order.append(new_o)\n",
    "    numeric = {v: i for (i, v) in enumerate(order)}\n",
    "    return numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the ordinal variables to numeric\n",
    "ordinal_replacement = {}\n",
    "for column in ordinal:\n",
    "    exp = data_dict.loc[column, 'expected_values']\n",
    "    if column in clean_up:\n",
    "        ordinal_replacement[column] = ordinal_assignment(exp, clean_up[column])\n",
    "    else:\n",
    "        ordinal_replacement[column] = ordinal_assignment(exp)\n",
    "    # SMJ: do the actual conversion of data. Make sure that case does not matter!\n",
    "    map_[column] = map_[column].apply(\n",
    "        # make sure string values are all in lower case, other types are passed as they are\n",
    "        lambda x: x.lower() if type(x) == str else x).replace(\n",
    "            # replace all those column values that are in the dict \"ordinal_replacement\"\n",
    "            # where key is the current string value and value is the new integer value.\n",
    "            # Ensure current strings are all small characters, via the map function.\n",
    "            list(map(str.lower, ordinal_replacement[column].keys())), \n",
    "            ordinal_replacement[column].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a combined list of columns for correlations between the numeric columns. We'll track both a condensed form and a full distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric = np.hstack([continous, np.array(list(ordinal)), np.array(datetime)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation = {}\n",
    "distance_matrix_long = {}\n",
    "trouble = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pairs = list(itertools.combinations(numeric, 2))\n",
    "sys.stderr.write(\"comparing %i pairs:\\n\" % len(pairs))\n",
    "for i, (c1, c2) in enumerate(pairs):\n",
    "    if i % int(len(pairs) / 10) == 0:\n",
    "        sys.stderr.write('  %.0f%%\\n' % (i*100/len(pairs)))\n",
    "    sub = map_[[c1, c2]].dropna()\n",
    "    spearman_r, spearman_p = scipy.stats.spearmanr(sub[c1], sub[c2])\n",
    "    pearson_r, pearson_p = scipy.stats.pearsonr(sub[c1], sub[c2])\n",
    "\n",
    "    summary = {'stat_': np.absolute(spearman_r),\n",
    "               'r_': np.absolute(pearson_r),\n",
    "               'p-value': pearson_p,\n",
    "               }\n",
    "\n",
    "    correlation[(c1, c2)] = summary\n",
    "    distance_matrix_long[(c1, c2)] = summary\n",
    "    distance_matrix_long[(c2, c1)] = summary\n",
    "sys.stderr.write(\"completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in numeric:\n",
    "    distance_matrix_long[(c, c)] = {'stat_': 1,\n",
    "                                    'r_': 1,\n",
    "                                    'p-value': 0,\n",
    "                                    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical varibales, we're going to calculate Cramer's v from a chi-square test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cramers_corrected_stat(chi2, confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher, \n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        \n",
    "        This is adapted from a stack overflow description:\n",
    "        \n",
    "        https://stackoverflow.com/questions/20892799/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix\n",
    "    \"\"\"\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by filtering low abundance groups, which do not have suffecient counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_counts(c, lower_counts):\n",
    "    counts = map_[c].value_counts()\n",
    "    drop = counts.index[counts < lower_counts]\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low = {c: {d : np.nan for d in check_counts(c, 25)}\n",
    "       for c in np.hstack([np.array(list(categorical)), np.array(list(ordinal))])\n",
    "       if len(check_counts(c, 25)) > 0\n",
    "       }\n",
    "map_.replace(low, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = np.hstack([np.array(list(categorical)), np.array(list(ordinal))])\n",
    "skips = [c for c in groups if np.min(map_[c].value_counts()) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll loop through the comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pairs = list(itertools.combinations(skips, 2))\n",
    "sys.stderr.write(\"comparing %i pairs:\\n\" % len(pairs))\n",
    "for i, ((c1, c2)) in enumerate(pairs):\n",
    "# for (c1, c2) in itertools.combinations(['acid_reflux', 'state', 'library_construction_protocol'], 2):\n",
    "    if i % int(len(pairs) / 10) == 0:\n",
    "        sys.stderr.write('  %.0f%%\\n' % (i*100/len(pairs)))\n",
    "    pivot = np.array([\n",
    "        [np.sum((map_[c1] == v1) & (map_[c2] == v2)) \n",
    "         for v2 in map_[c2].unique() if not pd.isnull(v2)]\n",
    "         for v1 in map_[c1].unique() if not pd.isnull(v1)\n",
    "        ])\n",
    "    pivot = pivot[:, pivot.sum(0) > 0][pivot.sum(1) > 0]\n",
    "    if not np.any(np.array(list(pivot.shape)) == 0):\n",
    "        chi2, p, _, _ = scipy.stats.chi2_contingency(pivot)\n",
    "        v = cramers_corrected_stat(chi2, pivot)\n",
    "    else:\n",
    "        chi2 = np.nan\n",
    "        p = np.nan,\n",
    "        v = 1\n",
    "    summary = {'chi2': chi2,\n",
    "               'p-value': p,\n",
    "               'r_': v}\n",
    "    correlation[(c1, c2)] = summary\n",
    "    distance_matrix_long[(c1, c2)] = summary\n",
    "    distance_matrix_long[(c2, c1)] = summary\n",
    "sys.stderr.write(\"completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in skips:\n",
    "    distance_matrix_long[(c, c)] = {'stat_': 1,\n",
    "                                    'r_': 1,\n",
    "                                    'p-value': 0,\n",
    "                                    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need a relationship between continous and categorical variables (we've already accounted for ordinal variables). For this, we'll calculate a t-statistic and then convert this to a correlation coeffeceint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_values = map_[c1].value_counts().index\n",
    "group_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = [map_.loc[map_[c1] == v, c2].dropna().values for v in group_values\n",
    "                  if len(map_.loc[map_[c1] == v, c2].dropna().values) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_pairs = len(categorical) * len(np.hstack([continous, datetime]))\n",
    "sys.stderr.write(\"comparing %i pairs:\\n\" % num_pairs)\n",
    "i = 0\n",
    "for c1 in categorical:\n",
    "    for c2 in np.hstack([continous, datetime]):\n",
    "        i += 1\n",
    "        if i % int(num_pairs / 10) == 0:\n",
    "            sys.stderr.write('  %.0f%%\\n' % (i*100/num_pairs))\n",
    "#         print(c1, c2)\n",
    "        group_values = map_[c1].value_counts().index\n",
    "        if len(group_values) < 2:\n",
    "            continue\n",
    "        groups = [map_.loc[map_[c1] == v, c2].dropna().values for v in group_values\n",
    "                  if len(map_.loc[map_[c1] == v, c2].dropna().values) > 0]\n",
    "        f_, p_ = scipy.stats.f_oneway(*groups)\n",
    "        df_n = len(np.hstack(groups)) - len(groups)\n",
    "        df_d = len(groups) - 1\n",
    "        r_ = (f_ / (f_*df_n + df_d))\n",
    "        summary = pd.Series({'stat': f_,\n",
    "                             'p-value': p_,\n",
    "                             'r_': r_,})\n",
    "        correlation[(c1, c2)] = summary\n",
    "        distance_matrix_long[(c1, c2)] = summary\n",
    "        distance_matrix_long[(c2, c1)] = summary\n",
    "sys.stderr.write(\"completed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the relationships defined and scaled, we'll convert the data to a saveable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation = pd.DataFrame.from_dict(correlation, orient='index')\n",
    "correlation.to_csv('./correlations.txt', sep='\\t', index_label=['var1', 'var2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll use the diagnonals to convert the data to a distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_matrix_long = pd.DataFrame.from_dict(distance_matrix_long, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm_corr = distance_matrix_long['r_'].unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll drop out anything that doesnt' have correlations in the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null1 = np.isnan(dm_corr).iloc[0] == False\n",
    "dm_corr1 = dm_corr.loc[null1, null1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll replace any correlations that remain with a value 0 (if the data is missing, there is no correaltion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm_corr1.replace(np.nan, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert the correlations to a distance matrix object, and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm = skbio.DistanceMatrix(data=(1 - dm_corr1.values), ids=dm_corr1.index)\n",
    "dm.write('./correlations_dm.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll perform hierarchical clustering on the distance matrix, to determine the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_ = skbio.tree.TreeNode.from_linkage_matrix(ward(dm.condensed_form()), dm.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cluster the objects using the correlations we've transformed into distance matrix. We will traverse the resulting tree from tip to node until we've accumlated a branch length longer than our threshhold (in this case, we've selected 0.29, which estimates a correlation of 0.5). We'll treat this values as the root of our new cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.29\n",
    "format_long = True\n",
    "\n",
    "seen_clusters = dict()\n",
    "f = open('agp_metadata_clusters_%f.txt' % threshold, 'w')\n",
    "f.write('# threshold = %f, ward clustering of %s matrix\\n' % (threshold, str(dm.shape)))\n",
    "for i, tip in enumerate(tree_.tips()):\n",
    "    curr_node = tip\n",
    "    tip_dist = tip.length\n",
    "    while tip_dist <= threshold:\n",
    "        curr_node = curr_node.parent\n",
    "        tip_dist += curr_node.length\n",
    "    \n",
    "    cluster = []\n",
    "    if curr_node.is_tip():\n",
    "        cluster = [curr_node.name]\n",
    "    else:\n",
    "        cluster = [t.name for t in curr_node.tips()]\n",
    "    cluster_id = \"\".join(cluster)\n",
    "    if cluster_id not in seen_clusters:\n",
    "        seen_clusters[cluster_id] = True\n",
    "        corr_matrix = dm.filter(cluster)\n",
    "        if format_long:\n",
    "            corr_matrix.write(f)\n",
    "        else:\n",
    "            f.write(\"\\t\".join(sorted(corr_matrix.ids)))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
